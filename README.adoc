ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:source-highlighter: rouge
:toc:
:toc-placement!:

= yakety-csv

The yakety-csv or _Yet-Another-Know-Everything-Tentative-Yap-CSV_ library.
It should work with any character separator, so `,`, `;` and `tabs`; even custom separators.
At this point it only works with single character line break: `\n`, `\r` or custom single character. Soon support for Windows `\r\n` will be added.

IMPORTANT: There are plenty implementations out there, please look for them, this is an exercise.

toc::[]

== Features

* Fully functional, Java Stream based API
* Configuration over code: +
"`library user defines rules not process`"
** Columns are defined for reuse (enum?)
* Full support to RFC4180 both strict and non-strict
* Support for different locales
* Support for quoted values
* On parsing:
** _Step 1_ list of ordered string values
** _Step 2_ list of string values by key
** _Step 3_ nullable values

=== Bucket list

* on parsing:
** _Step 4_ basic type coercion from string
** _Step 5_ Boxed variation on basic types
** _Step 6_ Support for date, time, datetime (timezone or not)
** _Step 7_ assembled value object __(maybe?)__
* Caching of parsing results (denormalized files tend to repeat a lot).
* Support for custom separators, line breaks and quotes __(maybe?)__
* in case there is a coercion error, return a marker that point to file, line, field where problem happened.

== Release Plans

* `v0.0.X` - empty project that compiles
* `v0.1.X` - stream String value of fields from a csv using strict RFC4180
* `v0.2.X` - Enable column configuration
* `v0.3.X`?- Proper handle of quotes
* [.line-through]#`v1.0.X` - stream Map.Entry of String values by column (RFC4180)#
* [.line-through]#`v1.1.X` - nullable columns (map stream cannot take null values)#
* [.line-through]#`v1.2.X` - proper limited parsing of int and double#
* [.line-through]#`v1.3.X` - proper parsing of long and float#
* [.line-through]#`v1.4.X` - proper parsing to Boxed numbers#
* [.line-through]#`v1.5.X` - proper parsing of Date, Time and DateTime#
* [.line-through]#`v1.6.X` - proper parsing of BigInteger and BigDecimal#
* [.line-through]#`v1.7.X` - support to domain values (enum) parsing#
* `v2.0.0` - Bean assembly from text by column map, includes:
    - safe parsing of standards types: int, long, float, double, BigInteger, BigDecimal, LocalDate, LocalDateTime, LocalTime.
    - parsing of domain based values backed by enums;
    - API for custom field parsing;
    - API that joins field parser and map key lookup;
    - any field that fails to be parsed is null;
    - all API avoids exceptions so stream is never broken;
    - API for a transformer from Map<K,String> to java bean.
    - API for indexed/id'ed beans
* `v2.0.1` - refactoring and test coverage improvements
* `v2.1.0` - joining 2 existing configurations, builder decides if it has column references or not, it is transparent for the API user.
* `v3.0.0` - Adding feature to identify parsing problems, includes:
    - Removal of all deprecated API;
    - implementation of Either;
    - a new parser and bean assembly that uses Either as output;
    - a new interface to represent a problem parsing a field, which can be applied along with ColumnDefinition;
    - introduction of result expectation vs actual result;
    - marker pointing to location where parsing failed.
* `v4.0.0` - first API stable version, includes:
    - optimizations
    - extra logging
    - integration test comments and documentation
    - package publication

== Tasks

. setup project:
- [x] gradle
- [x] spock tests
- [x] spock integration tests
- [x] git ignores
. functionalities:
- [x] simple csv to stream of fields
- [x] configurable parser
- [x] file format configuration
- [x] column definition interface
- [x] configurable csv columns to stream of String fieldByColumnName maps
- [x] indexed row value as field in map
- [x] use dynamic programming to check if line break is within quotes, ignore it if it is. should consume large files without blowing up the stack.
- [x] parser localization
- [x] column definition map to expected type (string for now)
- [x] from the map result apply identity type coercion to bean
- [ ] add coercion checks with bad results as separate dataset from raw values
- [ ] add null constraints
- [ ] configurable csv columns with type coercion (all types)
- [ ] configurable csv columns with type coercion to list of objects

== How to build

=== Environment setup requirements

Java 14 is needed, get it with SDKMan Gradle configuration recommended, ~/.gradle/gradle.properties:

[source,properties]
-----------------------------------------------------------
org.gradle.parallel=true
org.gradle.jvmargs=-Xmx2048M
org.gradle.caching=true
org.gradle.daemon.idletimeout=1800000
org.gradle.java.home=/home/user/.sdkman/candidates/java/14.0.2-open # <1>
-----------------------------------------------------------
<1> your own path for the JDK 15

=== _TL;DR_

[source,shell]
-----------------------------------------------------------
./gradlew
-----------------------------------------------------------

== How to use

The concept usage is that you are either:
- exploring data from a file you do not know the format or
- parsing well known CSV format multiple times from different files.

=== Simple CSV to Java `Stream<List<String>>`

[source, java]
-----------------------------------------------------------
final FileFormatConfiguration config =
    FileFormatConfiguration.builder().build()
final CsvParser textParser =
    org.shimomoto.yakety.csv.CsvParserFactory.toText(config)

final Stream<List<Stream>> textResults =
    textParser.parse(new FileInputSream(new File("that_data.csv")))
-----------------------------------------------------------

=== Simple CSV to `Stream<Map<String,String>>`

With added field for the line index, starting at 1 (headers were zero). The field name must not clash with a column name.

It is purely positional (does not check if first field matches first header column name), if you mess up the fields order, you mess up the mapping.

[source, java]
-----------------------------------------------------------
final FileFormatConfiguration config =
    FileFormatConfiguration.builder().build()
final CsvParser indexedMapParser =
    org.shimomoto.yakety.csv.CsvParserFactory.toRowIndexedTextMap(config, "#", List.of("colA","colB","colC"))

final Stream<Map<String,String>> textResults =
    indexedMapParser.parse(new FileInputSream(new File("that_data.csv")))

-----------------------------------------------------------

=== Simple CSV to `Stream<?>`

It builds upon the fields by column map with a dynamic index, those results are used to build a Java Bean.

A transformer from `Stream<Map<? extends ColumnDefinition,String>>` to whatever aggregate is to be used is needed.

[source, java]
-----------------------------------------------------------
final ExtendedFileFormatConfiguration config =
    ExtendedFileFormatConfiguration.builder()
        .indexColumn(MyVirtualColumns.INDX)
        .columns(MyColumns.INDX)
        .build()
final BeanAssembly<MyColumns, MyAggregate> transformer =
    new MyTransformer(Locale.EN)
final CsvParser beanParser =
    org.shimomoto.yakety.csv.CsvParserFactory.toBeans(config)

final Stream<MyAggregate> aggregates =
    beanParser.parse(new FileInputSream(new File("that_data.csv")))

-----------------------------------------------------------

=== TL;DR

Read the contents of link:src/integrationTest/groovy/org/shimomoto/yakety/csv/MarvelIT.groovy[MarvelIT.groovy] are creating and using multiple parsers.

If you just want to read from the test results:
[source, shell]
-----------------------------------------------------------
./gradlew integrationTest
-----------------------------------------------------------

then open link:build/reports/spock-reports/integrationTest/index.html[], these are the integration tests results

== Learning notes

. `Scanner` discards empty elements at beginning or end, which works ok when splitting lines, also being lazy is a must;
`String.split(/pattern/, -1)` works correctly (empty fields show up) but takes a `String` instead of `Pattern`; the `Pattern.split(/string/, -1)` works when the number of fields is unknown; when the number of fields is known just pass the number instead of a negative.
. [.line-through]#Regular expressions with matches and groups take more processing power, the lookahead doesn't and works as would the index based string walk.# +
The regular expression break by line blows up the stack; the solution I can think of is to consume lines, then check if there is an open quote, consume another line until all open quotes are closed, then it would be better to just already consume fields while at that.
. Java Pattern class cannot be used on hash or equals ðŸ¤·.

== Questions

. Should the `ColumnDefinition` be enforced at API level?
That would force split for String columns. +
Perhaps it should be enforced when types are to be used...
