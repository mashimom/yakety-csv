ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:source-highlighter: rouge
:toc:
:toc-placement!:

= yakety-csv

The yakety-csv or _Yet-Another-Know-Everything-Tentative-Yap-CSV_ library.

IMPORTANT: There are plenty implementations out there, please look for them, this is an exercise.

toc::[]

== Features

* Fully functional, Java Stream based API
* Configuration over code: +
"`library user defines rules not process`"
** Columns are defined for reuse (enum?)
* Full support to RFC4180 both strict and non-strict
* Support for different locales
* Support for quoted values
* On parsing:
** _Step 1_ list of ordered string values
** _Step 2_ list of string values by key
** _Step 3_ nullable values

=== Bucket list

* on parsing:
** _Step 4_ basic type coercion from string
** _Step 5_ Boxed variation on basic types
** _Step 6_ Support for date, time, datetime (timezone or not)
** _Step 7_ assembled value object __(maybe?)__
* Caching of parsing results (denormalized files tend to repeat a lot).
* Support for custom separators, line breaks and quotes __(maybe?)__
* in case there is a coercion error, return a marker that point to file, line, field where problem happened.

== Release Plans

* `v0.0.X` - empty project that compiles
* `v0.1.X` - stream String value of fields from a csv using strict RFC4180
* `v0.2.X` - Enable column configuration
* `v0.3.X`?- Proper handle of quotes
* `v1.0.X` - stream Map.Entry of String values by column (RFC4180)
* `v1.1.X` - nullable columns (map stream cannot take null values)
* `v1.2.X` - proper limited parsing of int and double
* `v1.3.X` - proper parsing of long and float
* `v1.4.X` - proper parsing to Boxed numbers
* `v1.5.X` - proper parsing of Date, Time and DateTime
* `v1.6.X` - proper parsing of BigInteger and BigDecimal
* `v1.7.X` - support to domain values (enum) parsing
* `v2.0.0` - custom file formats
* `v2.1.0` - custom parsers
* `v3.0.0` - marker pointing to location where parsing failed instead of error

== Tasks

. setup project:
- [x] gradle
- [x] spock tests
- [x] spock integration tests
- [x] git ignores
. functionalities:
- [x] simple csv to stream of fields
- [x] configurable parser
- [x] file format configuration
- [x] column definition interface
- [x] configurable csv columns to stream of String fieldByColumnName maps
- [x] indexed row value as field in map
- [x] use dynamic programming to check if line break is within quotes, ignore it if it is. should consume large files without blowing up the stack.
- [ ] parser localization
- [ ] column definition map to expected type (string for now)
- [ ] from the map result apply identity type coercion to bean
- [ ] add coercion checks with bad results as separate dataset from raw values
- [ ] add null constraints
- [ ] configurable csv columns with type coercion (all types)
- [ ] configurable csv columns with type coercion to list of objects

== How to build

=== Environment setup requirements

Java 14 is needed, get it with SDKMan Gradle configuration recommended, ~/.gradle/gradle.properties:

[source,properties]
-----------------------------------------------------------
org.gradle.parallel=true
org.gradle.jvmargs=-Xmx2048M
org.gradle.caching=true
org.gradle.daemon.idletimeout=1800000
org.gradle.java.home=/home/user/.sdkman/candidates/java/15.0.2-open # <1>
-----------------------------------------------------------
<1> your own path for the JDK 15

== _TL;DR_

[source,shell]
-----------------------------------------------------------
./gradlew
-----------------------------------------------------------

== Learning notes

. `Scanner` discards empty elements at beginning or end, which works ok when splitting lines, also being lazy is a must;
`String.split(/pattern/, -1)` works correctly (empty fields show up) but takes a `String` instead of `Pattern`; the `Pattern.split(/string/, -1)` works when the number of fields is unknown; when the number of fields is known just pass the number instead of a negative.
. [.line-through]#Regular expressions with matches and groups take more processing power, the lookahead doesn't and works as would the index based string walk.# +
The regular expression break by line blows up the stack; the solution I can think of is to consume lines, then check if there is an open quote, consume another line until all open quotes are closed, then it would be better to just already consume fields while at that.

== Questions

. Should the `ColumnDefinition` be enforced at API level?
That would force split for String columns. +
Perhaps it should be enforced when types are to be used...
